{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oRyHTZiK8YSS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1 report"
      ],
      "metadata": {
        "id": "mngnydsmEsDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import shakespeare\n",
        "from xml.etree import ElementTree\n",
        "import nltk\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('shakespeare')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqpsOGb17Rf8",
        "outputId": "ddf566dc-229f-45b7-a21b-8d3a6b62dd77"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]   Package shakespeare is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "\n",
        "I went through the data once, extracting all the information I'll need in the following tasks, storing it mostly in `play_info` dictionary, which contained `Play_info` class accesible through the play's name.\n",
        "\n",
        "Data for Task 3 is stored in `dream_lines_by_character` dictionary.\n",
        "\n",
        "I used the `xml` files instead of the provided text files."
      ],
      "metadata": {
        "id": "v33fow7nD3Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Play_info:\n",
        "    def __init__(self, name : str,\n",
        "                 tokens : list[str],\n",
        "                 types : set,\n",
        "                 spoken_lines : list[str],\n",
        "                 lines_per_act : list[int]\n",
        "                 ) -> None:\n",
        "        self.name = name\n",
        "        self.tokens = tokens\n",
        "        self.types = types\n",
        "        self.spoken_lines = spoken_lines\n",
        "        self.lines_per_act = lines_per_act"
      ],
      "metadata": {
        "id": "LnjaP-H47Sx8"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_names = shakespeare.fileids()\n",
        "play_info : dict[str, Play_info] = {}\n",
        "\n",
        "dream_lines_by_character = {}\n",
        "dream_speaker = \"\"\n",
        "\n",
        "# I am sorry for this disgusting piece of nested code, but considering the nested structure of xml files,\n",
        "# this is necesary evil.\n",
        "for play_name in play_names:\n",
        "    tree = ElementTree.parse(shakespeare.open(play_name))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    tokens = []\n",
        "    spoken_lines = []\n",
        "\n",
        "    lines_per_act = []\n",
        "\n",
        "    for book_part in root:\n",
        "\n",
        "        # ignore parts of the book that aren't part of the play\n",
        "        if book_part.tag != \"ACT\":\n",
        "            continue\n",
        "\n",
        "        for act_part in book_part:\n",
        "\n",
        "            # tokenize scene titles and introductory texts outside of scenes\n",
        "            if act_part.tag != \"SCENE\" and act_part.text != None:\n",
        "                tokens.extend(nltk.tokenize.word_tokenize(act_part.text))\n",
        "                continue\n",
        "\n",
        "            for scene_part in act_part:\n",
        "\n",
        "                # tokenize stage directions and descriptions outside of dialogues\n",
        "                if scene_part.tag != \"SPEECH\" and scene_part.text != None:\n",
        "                    tokens.extend(nltk.tokenize.word_tokenize(scene_part.text))\n",
        "                    continue\n",
        "\n",
        "                for speech_part in scene_part:\n",
        "\n",
        "                    # ignore empty speech parts\n",
        "                    if speech_part.text == None:\n",
        "                        continue\n",
        "\n",
        "                    # tokenize dialogue\n",
        "                    tokens.extend(nltk.tokenize.word_tokenize(speech_part.text))\n",
        "\n",
        "                    # collect spoken lines\n",
        "                    if speech_part.tag == \"LINE\":\n",
        "                        spoken_lines.append(speech_part.text)\n",
        "\n",
        "                    # separate lines of dream.xml by speaker (3A)\n",
        "                    if play_name == \"dream.xml\":\n",
        "                        if speech_part.tag == \"SPEAKER\":\n",
        "                            dream_speaker = speech_part.text\n",
        "                        if speech_part.tag == \"LINE\":\n",
        "                            if dream_speaker not in dream_lines_by_character:\n",
        "                                dream_lines_by_character[dream_speaker] = []\n",
        "                            dream_lines_by_character[dream_speaker].append(speech_part.text)\n",
        "\n",
        "        # cumulative sum of lines by act\n",
        "        lines_per_act.append(len(spoken_lines))\n",
        "\n",
        "    # extract types from tokens\n",
        "    types = set(tokens)\n",
        "    play_info[play_name] = Play_info(play_name, tokens, types, spoken_lines, lines_per_act)\n"
      ],
      "metadata": {
        "id": "v0AfN_yk7WXT"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "\n",
        "I looped through the collected data and found the play with the most tokens, types and lines."
      ],
      "metadata": {
        "id": "8jfq06cUmcjb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8nkXG6HmD4K",
        "outputId": "68cca3de-1ef0-424d-d377-8be7a748e42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39133\n",
            "hamlet.xml\n",
            "5171\n",
            "hamlet.xml\n",
            "3978\n",
            "hamlet.xml\n"
          ]
        }
      ],
      "source": [
        "max_tokens = 0\n",
        "max_tokens_play = \"\"\n",
        "max_types = 0\n",
        "max_types_play = \"\"\n",
        "max_lines = 0\n",
        "max_lines_play = \"\"\n",
        "\n",
        "for play_name in play_names:\n",
        "    info : Play_info = play_info[play_name]\n",
        "\n",
        "    if len(info.tokens) > max_tokens:\n",
        "        max_tokens_play = info.name\n",
        "        max_tokens = len(info.tokens)\n",
        "\n",
        "    if len(info.types) > max_types:\n",
        "        max_types_play = info.name\n",
        "        max_types = len(info.types)\n",
        "\n",
        "    if len(info.spoken_lines) > max_lines:\n",
        "        max_lines_play = info.name\n",
        "        max_lines = len(info.spoken_lines)\n",
        "\n",
        "print(max_tokens)\n",
        "print(max_tokens_play)\n",
        "print(max_types)\n",
        "print(max_types_play)\n",
        "print(max_lines)\n",
        "print(max_lines_play)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hamlet has the most tokens, types and spoken lines. It's by far the longest play, with\n",
        "- 39133 tokens,\n",
        "- 5171 types,\n",
        "- 2978 lines.\n"
      ],
      "metadata": {
        "id": "Pjbb1UnPDFKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "\n",
        "The analysis of *A Midsummer Night’s Dream*."
      ],
      "metadata": {
        "id": "jvySFYPcmzMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 A\n",
        "\n",
        "I already extracted the corpus for the lines spoken by each character in Task 1.\n",
        "\n",
        "The method of extraction can be used for any play in the corpus without knowing the characters’ names in advance."
      ],
      "metadata": {
        "id": "wGWXlP7NGeDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 B, C, D, E\n",
        "\n",
        "In the folowing four tasks, we are supposed to answer three questions for four different instances of the corpus of the lines spoken by each character of *A Midsummer Night’s Dream*.\n",
        "\n",
        "They are:\n",
        "- Which characters have the most lines?\n",
        "- What are the twenty most frequent words in the play, and how many times do they appear?\n",
        "- What are each character’s five most frequently used words?\n",
        "\n",
        "The functions below answers these questions."
      ],
      "metadata": {
        "id": "3rhOkCg6Hztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def twenty_most_frequent_words(words_by_character):\n",
        "    word_counts = {}\n",
        "    for character, words in words_by_character.items():\n",
        "        for word in words:\n",
        "            if word not in word_counts:\n",
        "                word_counts[word] = 0\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    return sorted(word_counts.items(), key=lambda x: - x[1])[:20]\n",
        "\n",
        "def five_most_frequent_words_by_character(words_by_character):\n",
        "    word_counts = {}\n",
        "    for character, words in words_by_character.items():\n",
        "        word_counts[character] = {}\n",
        "        for word in words:\n",
        "            if word not in word_counts[character]:\n",
        "                word_counts[character][word] = 0\n",
        "            word_counts[character][word] += 1\n",
        "\n",
        "    for character in word_counts.keys():\n",
        "        word_counts[character] = sorted(word_counts[character].items(), key=lambda x: - x[1])[:5]\n",
        "\n",
        "    return word_counts"
      ],
      "metadata": {
        "id": "jQ7UjdYYJUom"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will answer the first question\n",
        "- Which characters have the most lines?\n",
        "\n",
        "just once, the number of lines for character stays the same no matter how I preprocess the text."
      ],
      "metadata": {
        "id": "fSxXbxhQKazh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_lines = 0\n",
        "max_lines_character = \"\"\n",
        "midsummers_dream_words_by_character = {}\n",
        "\n",
        "for character, lines in dream_lines_by_character.items():\n",
        "    if len(lines) > max_lines:\n",
        "        max_lines = len(lines)\n",
        "        max_lines_character = character\n",
        "\n",
        "    midsummers_dream_words_by_character[character] = ' '.join(lines)\n",
        "\n",
        "print(\"The character with the most lines:\")\n",
        "print(max_lines)\n",
        "print(max_lines_character)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrqH_z_yKwUa",
        "outputId": "372a9669-c0e1-498d-d761-cc5cce750e91"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The character with the most lines:\n",
            "232\n",
            "THESEUS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply only tokenization\n",
        "\n",
        "I tried two ways of tokenizing, one way with the official nltk tokenizer, and the other with a regex `\\w+`.\n",
        "\n",
        "When using the official tokenizer, the most usual words are punctuations, which is interesting, but it doesn't give us any information, about how lemmatizing or stemming changes the most frequent words.\n",
        "\n",
        "That is why I chose to use the regex tokenizer for the most of the task."
      ],
      "metadata": {
        "id": "97ahOyWFLIjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nltk word tokenizer**"
      ],
      "metadata": {
        "id": "SmcML7ngMnS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midsummers_dream_words_by_character_tokenized = midsummers_dream_words_by_character.copy()\n",
        "\n",
        "max_lines = 0\n",
        "max_lines_character = \"\"\n",
        "\n",
        "for character, words in midsummers_dream_words_by_character_tokenized.items():\n",
        "    midsummers_dream_words_by_character_tokenized[character] = nltk.tokenize.word_tokenize(words)\n",
        "\n",
        "print(\"TWENTY MOST COMMON WORDS IN THE PLAY:\")\n",
        "print(twenty_most_frequent_words(midsummers_dream_words_by_character_tokenized))\n",
        "print()\n",
        "\n",
        "for character, words in five_most_frequent_words_by_character(midsummers_dream_words_by_character_tokenized).items():\n",
        "    print(character)\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJFfnhXqMHll",
        "outputId": "edfd44c3-fb82-45fb-9467-753fc70885ec"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWENTY MOST COMMON WORDS IN THE PLAY:\n",
            "[(',', 1740), ('.', 667), ('the', 473), ('I', 458), (';', 349), ('and', 335), (':', 275), ('to', 270), ('of', 245), ('you', 239), ('a', 232), ('in', 211), ('And', 196), ('is', 189), ('?', 177), ('me', 176), ('my', 175), ('!', 173), ('not', 173), ('with', 140)]\n",
            "\n",
            "THESEUS\n",
            "[(',', 160), ('.', 82), ('the', 67), ('and', 49), (';', 41)]\n",
            "HIPPOLYTA\n",
            "[(',', 18), ('.', 13), ('the', 11), ('a', 9), ('I', 8)]\n",
            "EGEUS\n",
            "[(',', 45), ('my', 16), ('of', 11), ('.', 11), ('her', 8)]\n",
            "HERMIA\n",
            "[(',', 119), ('.', 49), ('I', 44), ('!', 33), ('?', 33)]\n",
            "DEMETRIUS\n",
            "[(',', 121), ('.', 53), ('I', 46), ('the', 31), ('not', 25)]\n",
            "LYSANDER\n",
            "[(',', 162), ('I', 54), ('.', 52), (';', 38), ('to', 29)]\n",
            "HELENA\n",
            "[(',', 203), ('.', 60), ('you', 57), ('I', 54), ('me', 46)]\n",
            "QUINCE\n",
            "[(',', 76), ('.', 45), ('you', 29), ('and', 27), ('the', 24)]\n",
            "BOTTOM\n",
            "[(',', 170), ('.', 75), ('I', 75), ('the', 54), ('to', 50)]\n",
            "FLUTE\n",
            "[(',', 19), ('a', 9), ('.', 7), ('not', 5), ('he', 5)]\n",
            "STARVELING\n",
            "[('.', 5), (',', 3), ('I', 3), ('is', 2), ('of', 2)]\n",
            "SNOUT\n",
            "[(',', 4), ('.', 4), ('?', 4), ('a', 3), ('the', 3)]\n",
            "SNUG\n",
            "[(',', 6), ('the', 3), ('you', 2), ('if', 2), ('it', 2)]\n",
            "ALL\n",
            "[('That', 1), ('would', 1), ('hang', 1), ('us', 1), (',', 1)]\n",
            "PUCK\n",
            "[(',', 160), ('.', 52), ('the', 50), ('I', 39), ('and', 37)]\n",
            "Fairy\n",
            "[(',', 19), ('the', 8), (';', 6), (':', 6), ('and', 6)]\n",
            "OBERON\n",
            "[(',', 132), ('the', 63), ('.', 56), ('I', 35), ('And', 35)]\n",
            "TITANIA\n",
            "[(',', 130), ('the', 45), ('.', 34), ('and', 30), (';', 29)]\n",
            "PEASEBLOSSOM\n",
            "[('.', 3), ('Ready', 2), ('Hail', 1), (',', 1), ('mortal', 1)]\n",
            "COBWEB\n",
            "[('.', 2), ('And', 1), ('I.', 1), ('Hail', 1), ('!', 1)]\n",
            "MOTH\n",
            "[('And', 1), ('I.', 1), ('Hail', 1), ('!', 1)]\n",
            "MUSTARDSEED\n",
            "[('.', 2), ('And', 1), ('I.', 1), ('Hail', 1), ('!', 1)]\n",
            "HERNIA\n",
            "[('I', 1), ('understand', 1), ('not', 1), ('what', 1), ('you', 1)]\n",
            "PHILOSTRATE\n",
            "[(',', 22), ('is', 9), ('.', 7), (';', 6), ('it', 5)]\n",
            "Prologue\n",
            "[(',', 43), ('.', 15), ('you', 8), ('with', 7), ('is', 6)]\n",
            "Wall\n",
            "[(',', 14), ('this', 4), ('I', 4), ('the', 4), ('doth', 3)]\n",
            "Pyramus\n",
            "[(',', 65), ('!', 19), ('O', 17), ('I', 17), ('.', 12)]\n",
            "Thisbe\n",
            "[(',', 32), ('.', 12), ('my', 7), ('I', 5), ('?', 5)]\n",
            "Lion\n",
            "[(',', 10), ('lion', 3), ('that', 2), ('on', 2), ('in', 2)]\n",
            "Moonshine\n",
            "[('the', 8), ('moon', 5), (';', 5), (',', 5), ('lanthorn', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**regex `\\w+` tokenizer**"
      ],
      "metadata": {
        "id": "_Iyu1nELMyoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midsummers_dream_words_by_character_tokenized = midsummers_dream_words_by_character.copy()\n",
        "\n",
        "max_lines = 0\n",
        "max_lines_character = \"\"\n",
        "\n",
        "for character, words in midsummers_dream_words_by_character_tokenized.items():\n",
        "    midsummers_dream_words_by_character_tokenized[character] = re.findall( r'\\w+',words)\n",
        "\n",
        "print(\"TWENTY MOST COMMON WORDS IN THE PLAY:\")\n",
        "print(twenty_most_frequent_words(midsummers_dream_words_by_character_tokenized))\n",
        "print()\n",
        "\n",
        "for character, words in five_most_frequent_words_by_character(midsummers_dream_words_by_character_tokenized).items():\n",
        "    print(character)\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0vHRVfmM7Ko",
        "outputId": "d866fcf7-9204-4c78-f8c8-47ce5e2496c5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWENTY MOST COMMON WORDS IN THE PLAY:\n",
            "[('the', 473), ('I', 461), ('and', 335), ('to', 275), ('of', 245), ('you', 239), ('a', 233), ('in', 213), ('And', 196), ('is', 186), ('me', 176), ('my', 175), ('not', 160), ('with', 140), ('that', 139), ('s', 125), ('it', 117), ('your', 115), ('this', 115), ('will', 109)]\n",
            "\n",
            "THESEUS\n",
            "[('the', 67), ('and', 49), ('of', 40), ('to', 38), ('in', 35)]\n",
            "HIPPOLYTA\n",
            "[('the', 11), ('a', 9), ('I', 8), ('in', 6), ('of', 6)]\n",
            "EGEUS\n",
            "[('my', 16), ('of', 11), ('her', 8), ('to', 7), ('is', 7)]\n",
            "HERMIA\n",
            "[('I', 44), ('me', 29), ('my', 25), ('you', 25), ('to', 22)]\n",
            "DEMETRIUS\n",
            "[('I', 46), ('the', 31), ('not', 21), ('to', 18), ('me', 18)]\n",
            "LYSANDER\n",
            "[('I', 54), ('to', 30), ('the', 27), ('my', 24), ('of', 22)]\n",
            "HELENA\n",
            "[('you', 57), ('I', 54), ('me', 46), ('and', 36), ('the', 29)]\n",
            "QUINCE\n",
            "[('you', 29), ('and', 27), ('the', 24), ('a', 19), ('is', 17)]\n",
            "BOTTOM\n",
            "[('I', 75), ('the', 54), ('to', 50), ('a', 46), ('and', 40)]\n",
            "FLUTE\n",
            "[('a', 9), ('not', 5), ('he', 5), ('I', 4), ('Pyramus', 4)]\n",
            "STARVELING\n",
            "[('I', 3), ('is', 2), ('of', 2), ('Here', 1), ('Peter', 1)]\n",
            "SNOUT\n",
            "[('a', 3), ('the', 3), ('not', 2), ('lion', 2), ('play', 2)]\n",
            "SNUG\n",
            "[('the', 3), ('you', 2), ('if', 2), ('it', 2), ('is', 2)]\n",
            "ALL\n",
            "[('That', 1), ('would', 1), ('hang', 1), ('us', 1), ('every', 1)]\n",
            "PUCK\n",
            "[('the', 50), ('I', 39), ('and', 37), ('And', 27), ('a', 26)]\n",
            "Fairy\n",
            "[('the', 8), ('and', 6), ('I', 5), ('And', 5), ('you', 5)]\n",
            "OBERON\n",
            "[('the', 63), ('I', 35), ('And', 35), ('of', 30), ('and', 27)]\n",
            "TITANIA\n",
            "[('the', 45), ('and', 30), ('And', 24), ('in', 22), ('I', 21)]\n",
            "PEASEBLOSSOM\n",
            "[('Ready', 2), ('Hail', 1), ('mortal', 1), ('Peaseblossom', 1)]\n",
            "COBWEB\n",
            "[('And', 1), ('I', 1), ('Hail', 1), ('Cobweb', 1), ('Ready', 1)]\n",
            "MOTH\n",
            "[('And', 1), ('I', 1), ('Hail', 1)]\n",
            "MUSTARDSEED\n",
            "[('And', 1), ('I', 1), ('Hail', 1), ('Mustardseed', 1), ('Ready', 1)]\n",
            "HERNIA\n",
            "[('I', 1), ('understand', 1), ('not', 1), ('what', 1), ('you', 1)]\n",
            "PHILOSTRATE\n",
            "[('is', 9), ('it', 5), ('in', 5), ('d', 5), ('play', 4)]\n",
            "Prologue\n",
            "[('you', 8), ('with', 7), ('is', 6), ('and', 6), ('This', 5)]\n",
            "Wall\n",
            "[('this', 4), ('I', 4), ('the', 4), ('doth', 3), ('That', 3)]\n",
            "Pyramus\n",
            "[('O', 17), ('I', 17), ('wall', 9), ('is', 7), ('thy', 7)]\n",
            "Thisbe\n",
            "[('my', 7), ('I', 5), ('love', 4), ('O', 3), ('me', 3)]\n",
            "Lion\n",
            "[('lion', 4), ('that', 2), ('on', 2), ('in', 2), ('I', 2)]\n",
            "Moonshine\n",
            "[('the', 8), ('moon', 5), ('lanthorn', 3), ('to', 3), ('This', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply lemmatization over tokenization\n",
        "\n",
        "For lemmatizing the text, I used nltk Word Net Lemmatizer."
      ],
      "metadata": {
        "id": "36wMM4SuNaCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midsummers_dream_words_by_character_lemmatized = midsummers_dream_words_by_character_tokenized.copy()\n",
        "\n",
        "for character, words in midsummers_dream_words_by_character_lemmatized.items():\n",
        "    lemmatized = []\n",
        "    for word in words:\n",
        "        lemmatized.append(nltk.WordNetLemmatizer.lemmatize(self=nltk.WordNetLemmatizer ,word=word))\n",
        "\n",
        "    midsummers_dream_words_by_character_lemmatized[character] = lemmatized\n",
        "\n",
        "print(\"TWENTY MOST COMMON WORDS IN THE PLAY:\")\n",
        "print(twenty_most_frequent_words(midsummers_dream_words_by_character_lemmatized))\n",
        "print()\n",
        "\n",
        "for character, words in five_most_frequent_words_by_character(midsummers_dream_words_by_character_lemmatized).items():\n",
        "    print(character)\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSHV3QOKOAJa",
        "outputId": "14b73ff5-b939-46ed-b8f9-38894eb4f7a7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWENTY MOST COMMON WORDS IN THE PLAY:\n",
            "[('the', 473), ('I', 461), ('and', 335), ('a', 328), ('to', 275), ('of', 245), ('you', 239), ('in', 213), ('And', 196), ('is', 186), ('me', 176), ('my', 175), ('not', 160), ('with', 140), ('that', 139), ('s', 125), ('it', 117), ('your', 115), ('this', 115), ('love', 114)]\n",
            "\n",
            "THESEUS\n",
            "[('the', 67), ('and', 49), ('of', 40), ('to', 38), ('a', 36)]\n",
            "HIPPOLYTA\n",
            "[('the', 11), ('a', 9), ('I', 8), ('in', 6), ('of', 6)]\n",
            "EGEUS\n",
            "[('my', 16), ('of', 11), ('her', 8), ('to', 7), ('is', 7)]\n",
            "HERMIA\n",
            "[('I', 44), ('me', 29), ('my', 25), ('you', 25), ('to', 22)]\n",
            "DEMETRIUS\n",
            "[('I', 46), ('the', 31), ('not', 21), ('to', 18), ('me', 18)]\n",
            "LYSANDER\n",
            "[('I', 54), ('a', 31), ('to', 30), ('the', 27), ('my', 24)]\n",
            "HELENA\n",
            "[('you', 57), ('I', 54), ('me', 46), ('a', 39), ('and', 36)]\n",
            "QUINCE\n",
            "[('you', 29), ('and', 27), ('a', 25), ('the', 24), ('is', 17)]\n",
            "BOTTOM\n",
            "[('I', 75), ('the', 54), ('a', 51), ('to', 50), ('and', 40)]\n",
            "FLUTE\n",
            "[('a', 11), ('not', 5), ('he', 5), ('I', 4), ('Pyramus', 4)]\n",
            "STARVELING\n",
            "[('I', 3), ('is', 2), ('of', 2), ('Here', 1), ('Peter', 1)]\n",
            "SNOUT\n",
            "[('a', 3), ('the', 3), ('not', 2), ('lion', 2), ('play', 2)]\n",
            "SNUG\n",
            "[('the', 3), ('you', 2), ('if', 2), ('it', 2), ('is', 2)]\n",
            "ALL\n",
            "[('That', 1), ('would', 1), ('hang', 1), ('u', 1), ('every', 1)]\n",
            "PUCK\n",
            "[('the', 50), ('I', 39), ('and', 37), ('a', 28), ('And', 27)]\n",
            "Fairy\n",
            "[('the', 8), ('and', 6), ('I', 5), ('And', 5), ('you', 5)]\n",
            "OBERON\n",
            "[('the', 63), ('I', 35), ('And', 35), ('of', 30), ('a', 28)]\n",
            "TITANIA\n",
            "[('the', 45), ('and', 30), ('And', 24), ('in', 22), ('I', 21)]\n",
            "PEASEBLOSSOM\n",
            "[('Ready', 2), ('Hail', 1), ('mortal', 1), ('Peaseblossom', 1)]\n",
            "COBWEB\n",
            "[('And', 1), ('I', 1), ('Hail', 1), ('Cobweb', 1), ('Ready', 1)]\n",
            "MOTH\n",
            "[('And', 1), ('I', 1), ('Hail', 1)]\n",
            "MUSTARDSEED\n",
            "[('And', 1), ('I', 1), ('Hail', 1), ('Mustardseed', 1), ('Ready', 1)]\n",
            "HERNIA\n",
            "[('I', 1), ('understand', 1), ('not', 1), ('what', 1), ('you', 1)]\n",
            "PHILOSTRATE\n",
            "[('is', 9), ('it', 5), ('in', 5), ('d', 5), ('a', 4)]\n",
            "Prologue\n",
            "[('you', 8), ('with', 7), ('is', 6), ('and', 6), ('This', 5)]\n",
            "Wall\n",
            "[('this', 4), ('I', 4), ('a', 4), ('the', 4), ('doth', 3)]\n",
            "Pyramus\n",
            "[('O', 17), ('I', 17), ('wall', 9), ('is', 7), ('thy', 7)]\n",
            "Thisbe\n",
            "[('my', 7), ('I', 5), ('love', 4), ('a', 4), ('O', 3)]\n",
            "Lion\n",
            "[('lion', 4), ('that', 2), ('on', 2), ('in', 2), ('I', 2)]\n",
            "Moonshine\n",
            "[('the', 8), ('moon', 5), ('lanthorn', 3), ('to', 3), ('This', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply stemming over tokenization\n",
        "\n",
        "I am using nltk Snowball stemmer for English."
      ],
      "metadata": {
        "id": "O9CbknK6OScw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midsummers_dream_words_by_character_stemmed = midsummers_dream_words_by_character_tokenized.copy()\n",
        "\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "\n",
        "for character, words in midsummers_dream_words_by_character_stemmed.items():\n",
        "    stemmed = []\n",
        "    for word in words:\n",
        "        stemmed.append(stemmer.stem(word))\n",
        "\n",
        "    midsummers_dream_words_by_character_stemmed[character] = stemmed\n",
        "\n",
        "print(\"TWENTY MOST COMMON WORDS IN THE PLAY:\")\n",
        "print(twenty_most_frequent_words(midsummers_dream_words_by_character_stemmed))\n",
        "print()\n",
        "\n",
        "for character, words in five_most_frequent_words_by_character(midsummers_dream_words_by_character_stemmed).items():\n",
        "    print(character)\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k2sFW_AOj8X",
        "outputId": "3e466c8a-145e-4232-9d95-681b0c4fddcc"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWENTY MOST COMMON WORDS IN THE PLAY:\n",
            "[('the', 546), ('and', 531), ('i', 464), ('to', 336), ('you', 274), ('of', 262), ('a', 260), ('in', 240), ('my', 199), ('is', 194), ('that', 186), ('me', 176), ('not', 170), ('with', 168), ('this', 148), ('for', 142), ('love', 136), ('it', 133), ('your', 127), ('s', 125)]\n",
            "\n",
            "THESEUS\n",
            "[('the', 82), ('and', 61), ('to', 49), ('of', 45), ('in', 40)]\n",
            "HIPPOLYTA\n",
            "[('the', 12), ('a', 9), ('and', 8), ('i', 8), ('of', 7)]\n",
            "EGEUS\n",
            "[('my', 18), ('of', 13), ('and', 11), ('to', 8), ('her', 8)]\n",
            "HERMIA\n",
            "[('i', 44), ('and', 30), ('me', 29), ('my', 28), ('you', 28)]\n",
            "DEMETRIUS\n",
            "[('i', 46), ('the', 35), ('and', 24), ('to', 23), ('not', 22)]\n",
            "LYSANDER\n",
            "[('i', 54), ('and', 39), ('to', 34), ('the', 34), ('my', 27)]\n",
            "HELENA\n",
            "[('you', 67), ('i', 55), ('and', 54), ('me', 46), ('to', 41)]\n",
            "QUINCE\n",
            "[('you', 33), ('and', 27), ('the', 25), ('a', 20), ('is', 18)]\n",
            "BOTTOM\n",
            "[('i', 75), ('the', 62), ('to', 51), ('a', 48), ('and', 46)]\n",
            "FLUTE\n",
            "[('a', 9), ('not', 5), ('he', 5), ('i', 4), ('most', 4)]\n",
            "STARVELING\n",
            "[('i', 3), ('out', 2), ('is', 2), ('he', 2), ('of', 2)]\n",
            "SNOUT\n",
            "[('a', 3), ('the', 3), ('not', 2), ('lion', 2), ('play', 2)]\n",
            "SNUG\n",
            "[('the', 3), ('you', 2), ('if', 2), ('it', 2), ('is', 2)]\n",
            "ALL\n",
            "[('that', 1), ('would', 1), ('hang', 1), ('us', 1), ('everi', 1)]\n",
            "PUCK\n",
            "[('and', 64), ('the', 54), ('i', 39), ('a', 32), ('that', 21)]\n",
            "Fairy\n",
            "[('and', 11), ('the', 9), ('you', 6), ('i', 5), ('over', 4)]\n",
            "OBERON\n",
            "[('the', 66), ('and', 62), ('i', 35), ('with', 32), ('of', 31)]\n",
            "TITANIA\n",
            "[('the', 58), ('and', 54), ('to', 26), ('with', 23), ('in', 22)]\n",
            "PEASEBLOSSOM\n",
            "[('readi', 2), ('hail', 1), ('mortal', 1), ('peaseblossom', 1)]\n",
            "COBWEB\n",
            "[('and', 1), ('i', 1), ('hail', 1), ('cobweb', 1), ('readi', 1)]\n",
            "MOTH\n",
            "[('and', 1), ('i', 1), ('hail', 1)]\n",
            "MUSTARDSEED\n",
            "[('and', 1), ('i', 1), ('hail', 1), ('mustardse', 1), ('readi', 1)]\n",
            "HERNIA\n",
            "[('i', 1), ('understand', 1), ('not', 1), ('what', 1), ('you', 1)]\n",
            "PHILOSTRATE\n",
            "[('is', 9), ('it', 6), ('which', 5), ('in', 5), ('d', 5)]\n",
            "Prologue\n",
            "[('and', 10), ('you', 9), ('with', 7), ('to', 7), ('is', 6)]\n",
            "Wall\n",
            "[('and', 6), ('this', 5), ('wall', 5), ('that', 4), ('i', 4)]\n",
            "Pyramus\n",
            "[('o', 17), ('i', 17), ('wall', 9), ('and', 8), ('thi', 8)]\n",
            "Thisbe\n",
            "[('my', 10), ('i', 5), ('come', 5), ('and', 4), ('love', 4)]\n",
            "Lion\n",
            "[('lion', 4), ('you', 2), ('the', 2), ('that', 2), ('on', 2)]\n",
            "Moonshine\n",
            "[('the', 8), ('moon', 5), ('this', 4), ('lanthorn', 3), ('i', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove stopwords\n",
        "\n",
        "I am removing stopwords from the tokenized version of the text and using the standard nltk stopwords list."
      ],
      "metadata": {
        "id": "mUlZnqo_Ov6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midsummers_dream_words_by_character_filtered = midsummers_dream_words_by_character_tokenized.copy()\n",
        "\n",
        "for character, words in midsummers_dream_words_by_character_filtered.items():\n",
        "    filtered = []\n",
        "    for word in words:\n",
        "        if word.lower() not in stopwords.words('english'):\n",
        "            filtered.append(word)\n",
        "\n",
        "    midsummers_dream_words_by_character_filtered[character] = filtered\n",
        "\n",
        "print(\"TWENTY MOST COMMON WORDS IN THE PLAY:\")\n",
        "print(twenty_most_frequent_words(midsummers_dream_words_by_character_filtered))\n",
        "print()\n",
        "\n",
        "for character, words in five_most_frequent_words_by_character(midsummers_dream_words_by_character_filtered).items():\n",
        "    print(character)\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07G3Y9T2O9Zi",
        "outputId": "36495cde-f6dd-448c-ce78-5b49a687ac50"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWENTY MOST COMMON WORDS IN THE PLAY:\n",
            "[('love', 105), ('thou', 98), ('shall', 65), ('thee', 64), ('thy', 56), ('man', 50), ('night', 49), ('Pyramus', 44), ('Hermia', 43), ('sweet', 43), ('must', 42), ('Demetrius', 40), ('eyes', 39), ('come', 39), ('Lysander', 39), ('see', 38), ('one', 37), ('good', 36), ('would', 36), ('play', 35)]\n",
            "\n",
            "THESEUS\n",
            "[('shall', 10), ('love', 7), ('play', 7), ('moon', 6), ('come', 6)]\n",
            "HIPPOLYTA\n",
            "[('night', 3), ('moon', 3), ('Four', 2), ('quickly', 2), ('like', 2)]\n",
            "EGEUS\n",
            "[('Demetrius', 5), ('lord', 4), ('Lysander', 4), ('love', 4), ('duke', 3)]\n",
            "HERMIA\n",
            "[('Lysander', 21), ('thou', 13), ('love', 11), ('eyes', 6), ('low', 6)]\n",
            "DEMETRIUS\n",
            "[('thou', 12), ('thy', 10), ('love', 9), ('thee', 9), ('lord', 8)]\n",
            "LYSANDER\n",
            "[('love', 21), ('thee', 16), ('Hermia', 14), ('Helena', 13), ('thou', 13)]\n",
            "HELENA\n",
            "[('Hermia', 15), ('love', 15), ('Demetrius', 14), ('one', 11), ('eyes', 9)]\n",
            "QUINCE\n",
            "[('Pyramus', 15), ('play', 10), ('must', 10), ('Thisby', 8), ('Bottom', 7)]\n",
            "BOTTOM\n",
            "[('good', 15), ('man', 14), ('let', 11), ('make', 10), ('Pyramus', 9)]\n",
            "FLUTE\n",
            "[('Pyramus', 4), ('sixpence', 4), ('day', 4), ('would', 3), ('play', 2)]\n",
            "STARVELING\n",
            "[('Peter', 1), ('Quince', 1), ('believe', 1), ('must', 1), ('leave', 1)]\n",
            "SNOUT\n",
            "[('lion', 2), ('play', 2), ('Bottom', 2), ('Peter', 1), ('Quince', 1)]\n",
            "SNUG\n",
            "[('lion', 1), ('part', 1), ('written', 1), ('pray', 1), ('give', 1)]\n",
            "ALL\n",
            "[('would', 1), ('hang', 1), ('us', 1), ('every', 1), ('mother', 1)]\n",
            "PUCK\n",
            "[('thou', 8), ('shall', 7), ('night', 6), ('Athenian', 6), ('must', 6)]\n",
            "Fairy\n",
            "[('Thorough', 2), ('thorough', 2), ('fairy', 2), ('queen', 2), ('gone', 2)]\n",
            "OBERON\n",
            "[('thou', 26), ('love', 17), ('thy', 11), ('shall', 11), ('Titania', 8)]\n",
            "TITANIA\n",
            "[('thee', 11), ('thy', 10), ('thou', 10), ('love', 9), ('fairy', 7)]\n",
            "PEASEBLOSSOM\n",
            "[('Ready', 2), ('Hail', 1), ('mortal', 1), ('Peaseblossom', 1)]\n",
            "COBWEB\n",
            "[('Hail', 1), ('Cobweb', 1), ('Ready', 1)]\n",
            "MOTH\n",
            "[('Hail', 1)]\n",
            "MUSTARDSEED\n",
            "[('Hail', 1), ('Mustardseed', 1), ('Ready', 1)]\n",
            "HERNIA\n",
            "[('understand', 1), ('mean', 1)]\n",
            "PHILOSTRATE\n",
            "[('play', 4), ('lord', 4), ('brief', 2), ('ten', 2), ('words', 2)]\n",
            "Prologue\n",
            "[('know', 4), ('man', 4), ('Thisby', 4), ('Wall', 4), ('come', 3)]\n",
            "Wall\n",
            "[('doth', 3), ('wall', 3), ('lovers', 2), ('whisper', 2), ('Wall', 2)]\n",
            "Pyramus\n",
            "[('wall', 9), ('thy', 7), ('Thisby', 6), ('see', 6), ('die', 6)]\n",
            "Thisbe\n",
            "[('love', 4), ('lips', 3), ('Come', 3), ('wall', 2), ('often', 2)]\n",
            "Lion\n",
            "[('lion', 4), ('ladies', 1), ('whose', 1), ('gentle', 1), ('hearts', 1)]\n",
            "Moonshine\n",
            "[('moon', 5), ('lanthorn', 3), ('doth', 2), ('horned', 2), ('present', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What do you notice about how the answers change? (3F)\n",
        "\n",
        "There is a really big difference based on the tokenizer which is used, but that really just comes down to the question of what is a word.\n",
        "\n",
        "From the other experiments, the biggest impact had the stopwords removal for sure. It makes sence, that removing the most used words in a language would greatly impact the top most repeated words of a play in that language.\n",
        "\n",
        "Second biggest impact had stemming, but that probably is because the stemmer I used also lowercased all of the words. Because of that, *and* and *And* became one word and got a bit higher in the list, along with some other words.\n",
        "\n",
        "The difference between just tokenizing and tokenizing and lemmatizing is almost nothing.\n",
        "\n",
        "These observations could be connected to the fact that most of the top words are actually stopwords, and that a lot of stopwords have actually just one form, so their original, stemmed and lemmatized forms are one and the same."
      ],
      "metadata": {
        "id": "4TOZ382oPgm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 G\n",
        "\n",
        "Even though stopwords removal gave the best results, it could've been even better. I was actually trying to find Shakespearian stopwords list, but I wasn't succesful.\n",
        "\n",
        "As for the second part, many verbs have the form ending with *-st*, e.g. *hast* or *dost*. Those aren't recognized by modern stemmers nor lemmatizers. Another example is with the *-th* ending: *hath*, *doth*. Also the *thou*, *thee* and *thy* aren't known by modern English lemmatizers.\n",
        "\n",
        "We could add rules for dealing with the verbs, because the same as we now have rules for word endings, there are rules for verb endings from shakespearian times."
      ],
      "metadata": {
        "id": "BvDPI-pBSf63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        "\n",
        "N gram model."
      ],
      "metadata": {
        "id": "Kk3hmDpe8M07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ngram_model:\n",
        "    \"\"\"N gram model.\n",
        "\n",
        "    Needs to be trained by calling 'fit()' before usage.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N : int, unit : str = \"word\") -> None:\n",
        "        \"\"\"N : number of consecutive units taken into account.\n",
        "         In the range of 1 to number of units in a training test.\n",
        "\n",
        "        unit = {'word', 'char'} : basic unit for segmenting text\"\"\"\n",
        "\n",
        "        self.N = N\n",
        "        self.unit = unit\n",
        "        self.probabilities_dict = {}\n",
        "\n",
        "        if self.unit != \"word\" and self.unit != \"char\":\n",
        "            raise ValueError(\"Unknown argument: \" + self.unit + \". Choose 'word' or 'char' as a unit.\")\n",
        "\n",
        "    def fit(self, texts : list[str]) -> None:\n",
        "        \"\"\"text: source for training the model\"\"\"\n",
        "\n",
        "        if self.unit == \"word\":\n",
        "            tokenized_text = []\n",
        "            for text in texts:\n",
        "                tokenized_text += [\"<s>\" for padding_i in range(self.N - 1)] + nltk.tokenize.word_tokenize(text)\n",
        "        elif self.unit == \"char\":\n",
        "            tokenized_text = []\n",
        "            for text in texts:\n",
        "                tokenized_text += [\"<s>\" for padding_i in range(self.N - 1)] + [character for character in text]\n",
        "        else:\n",
        "            raise ValueError(\"Unknown unit: \" + self.unit + \".\")\n",
        "\n",
        "        occurences_of_context = {}\n",
        "\n",
        "        for i in range(self.N - 1, len(tokenized_text)):\n",
        "            context = tuple([tokenized_text[context_i] for context_i in range(i - self.N + 1, i)])\n",
        "            target = tokenized_text[i]\n",
        "\n",
        "            # count occurences of context\n",
        "            if context not in occurences_of_context:\n",
        "                occurences_of_context[context] = 1\n",
        "            else:\n",
        "                occurences_of_context[context] += 1\n",
        "\n",
        "            # count occurences of N-grams\n",
        "            if context not in self.probabilities_dict:\n",
        "                self.probabilities_dict[context] = {}\n",
        "\n",
        "            if target not in self.probabilities_dict[context]:\n",
        "                self.probabilities_dict[context][target] = 1\n",
        "            else:\n",
        "                self.probabilities_dict[context][target] += 1\n",
        "\n",
        "        # get probabilities from counts\n",
        "        for context in self.probabilities_dict.keys():\n",
        "            for target in self.probabilities_dict[context].keys():\n",
        "                self.probabilities_dict[context][target] /= occurences_of_context[context]\n",
        "\n",
        "    def predict(self, length : int) -> list[str]:\n",
        "        \"\"\"Predicts text with 'length' of units.\"\"\"\n",
        "\n",
        "        context = tuple([\"<s>\" for context_i in range(self.N - 1)])\n",
        "        generated_text = []\n",
        "\n",
        "        while len(generated_text) < length:\n",
        "            next_unit = self._next_unit(context)\n",
        "\n",
        "            context_addition = []\n",
        "            if self.N > 1 :\n",
        "                context_addition.append(next_unit)\n",
        "            context = tuple([context[context_i + 1] for context_i in range(self.N - 2)] + context_addition)\n",
        "\n",
        "            if next_unit != \"<s>\":\n",
        "                generated_text.append(next_unit)\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    def _next_unit(self, context : tuple[str]) -> str:\n",
        "        if context not in self.probabilities_dict:\n",
        "            return \"<s>\"\n",
        "\n",
        "        units = []\n",
        "        probabilities = []\n",
        "\n",
        "        for unit, probability in self.probabilities_dict[context].items():\n",
        "            units.append(unit)\n",
        "            probabilities.append(probability)\n",
        "\n",
        "        return random.choices(units, probabilities)[0]"
      ],
      "metadata": {
        "id": "AbUI2UnT8OBZ"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4 A"
      ],
      "metadata": {
        "id": "1eRNlUOLXWoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_spoken_lines = []\n",
        "for play_name in play_names:\n",
        "    all_spoken_lines += play_info[play_name].spoken_lines"
      ],
      "metadata": {
        "id": "bHcvmFTjT1Aj"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word unigrams**"
      ],
      "metadata": {
        "id": "uf85zn1VTG8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(1, \"word\")\n",
        "model.fit(all_spoken_lines)\n",
        "prediction = model.predict(20)\n",
        "\n",
        "print(\" \".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUmukyYZ8Uww",
        "outputId": "8954c9d9-c956-45f4-8cd6-eb851feb803a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", bring heap says Was , for ! That I you second shall The a not stabbed , chide ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word bigrams**"
      ],
      "metadata": {
        "id": "NE6J1u72TZXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(2, \"word\")\n",
        "model.fit(all_spoken_lines)\n",
        "prediction = model.predict(20)\n",
        "\n",
        "print(\" \".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIK7i59QTgfg",
        "outputId": "c83838a5-0675-4f1a-f3bb-5ce395f565b7"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stands he well done , and being down , When good Tubal ; for her 'T is -- a heavy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word trigrams**"
      ],
      "metadata": {
        "id": "W4D7cWa3UBBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(3, \"word\")\n",
        "model.fit(all_spoken_lines)\n",
        "prediction = model.predict(20)\n",
        "\n",
        "print(\" \".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qC8bsNLUDfd",
        "outputId": "d083f5db-7a8e-4aa6-f96b-0fb0baf6914c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your servants ever And bring these gentlemen , And with such familiar instances , name ? Whither am I mad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigger the N, the more fluent the generated sentence sounds."
      ],
      "metadata": {
        "id": "ZEeQXHm4WDBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4 B **Generating just from the first 50 lines of Hamlet**"
      ],
      "metadata": {
        "id": "EtOD9wrYVuvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(3, \"word\")\n",
        "model.fit(play_info[\"hamlet.xml\"].spoken_lines[:50])\n",
        "prediction = model.predict(20)\n",
        "\n",
        "print(\" \".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl4Eo2UuV5Ey",
        "outputId": "a25ef131-fdaa-4cf4-c3e3-faa02e2ed19f"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What we have two nights seen . Holla ! Bernardo ! He may approve our eyes and speak to it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have very little text to learn from, there wont be many possible continuation to a given context, so the model will just repeat the sentences from the learned material. The bigger the model (the more learning material is available to the model), the more continuations for a given context. Therefore the probability of the model repeating sentences from the source, or even repeating itself are much lower.\n",
        "\n",
        "In the above example we can see, that most of the line is a copy of a line from the beginning of Hamlet."
      ],
      "metadata": {
        "id": "huLQhRu1WYby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4 C\n",
        "\n",
        "What happens if you generate text character-by-character (letter-by-letter) instead of word-by-word?\n",
        "\n",
        "New words that sound like english but have no meaning happen!"
      ],
      "metadata": {
        "id": "kuNJNqo8XcZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**character bigrams**"
      ],
      "metadata": {
        "id": "9UOW9NiZXt8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(2, \"char\")\n",
        "model.fit(all_spoken_lines)\n",
        "prediction = model.predict(100)\n",
        "\n",
        "print(\"\".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goZsLQk2Xo8y",
        "outputId": "33205e6b-b521-4b73-e0b1-a7705b359998"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thas, ngose he ad sod the ounthis thespeansthelerobe chy, mowhea; mbes.Whave w!Theve omuar ovipse, t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**character trigrams**"
      ],
      "metadata": {
        "id": "Sqfky601X5bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(3, \"char\")\n",
        "model.fit(all_spoken_lines)\n",
        "prediction = model.predict(100)\n",
        "\n",
        "print(\"\".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3y0s9flX8WX",
        "outputId": "a5cbb419-d574-4ea6-8bf1-c0232fdcf4b7"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thalty rethe suff. And thatere, my wasou suciall nowThe damet wore your firs tan thall your hat.How;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**character 4-grams**"
      ],
      "metadata": {
        "id": "cTsjCek5YB0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Ngram_model(4, \"char\")\n",
        "model.fit(all_spoken_lines)\n",
        "prediction = model.predict(100)\n",
        "\n",
        "print(\"\".join(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb6ZJEr0YNcQ",
        "outputId": "be98fde5-99bd-45be-8821-2ebc9c45bd1e"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There acturesar els,More me,Post lie to ence of thou the withat cue.I am in they to come servicessiu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5\n",
        "\n",
        "Naive Bayes model."
      ],
      "metadata": {
        "id": "oRyHTZiK8YSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Naive_bayes_model:\n",
        "    \"\"\"Naive Bayes model.\n",
        "\n",
        "    Needs to be trained by calling 'fit()' before usage.\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        self.categories = 0\n",
        "        self.tokens_in_category = {}\n",
        "        self.category_probability = []\n",
        "        self.category_size_in_tokens = []\n",
        "        self.vocab = set()\n",
        "\n",
        "    def fit(self, data: list[list[str]]) -> None:\n",
        "        \"\"\"data: Training data.\n",
        "\n",
        "        List of lists of data of each category.\n",
        "\n",
        "        Number of categories determined by the length of 'data'.\n",
        "        \"\"\"\n",
        "        # init\n",
        "        categories = len(data)\n",
        "        self.categories = categories\n",
        "        self.category_size_in_tokens = [0 for i in range(categories)]\n",
        "        self.category_probability = [0 for i in range(categories)]\n",
        "\n",
        "        for category in range(categories):\n",
        "            # init dictionary for counting token frequencies\n",
        "            self.tokens_in_category[category] : dict[str, int] = {}\n",
        "\n",
        "            for line in data[category]:\n",
        "                # count lines of data for each category\n",
        "                self.category_probability[category] += 1\n",
        "\n",
        "                for token in nltk.tokenize.word_tokenize(line):\n",
        "                    if token not in self.vocab:\n",
        "                        self.vocab.add(token)\n",
        "\n",
        "                    if token not in self.tokens_in_category[category]:\n",
        "                        self.tokens_in_category[category][token] = 0\n",
        "                    self.tokens_in_category[category][token] += 1\n",
        "\n",
        "                    self.category_size_in_tokens[category] += 1\n",
        "\n",
        "        # number of data in all categories\n",
        "        sum_of_categories = sum(self.category_probability)\n",
        "\n",
        "        # calculate probabilities\n",
        "        for category in range(categories):\n",
        "            self.category_probability[category] /= sum_of_categories\n",
        "\n",
        "\n",
        "    def predict(self, line : str) -> int:\n",
        "        \"\"\"Predict a class of the input line.\"\"\"\n",
        "\n",
        "        # init 0 probability for each category\n",
        "        predictions = [0 for i in range(self.categories)]\n",
        "\n",
        "        for category in range(self.categories):\n",
        "            # init prediction with category probability\n",
        "            prediction = self.category_probability[category]\n",
        "\n",
        "            for token in nltk.tokenize.word_tokenize(line):\n",
        "                # ignore tokes not included in vocabulary\n",
        "                if token not in self.vocab:\n",
        "                    continue\n",
        "\n",
        "                token_count_in_category = 0\n",
        "                if token in self.tokens_in_category[category]:\n",
        "                    token_count_in_category = self.tokens_in_category[category][token]\n",
        "\n",
        "                # multiply the prediction by smoothed probability of a word being in the category\n",
        "                prediction *= ((token_count_in_category + 1) / (self.category_size_in_tokens[category] + len(self.vocab)))\n",
        "\n",
        "            predictions[category] = prediction\n",
        "\n",
        "        return np.argmax(predictions)"
      ],
      "metadata": {
        "id": "X51exYAZ8X3G"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model : Naive_bayes_model, test_data : list[list[str]]) -> None:\n",
        "    overall_correct = 0\n",
        "    overall_length = 0\n",
        "\n",
        "    for category in range(len(test_data)):\n",
        "        correct = 0\n",
        "        for test_line in test_data[category]:\n",
        "            prediction = model.predict(test_line)\n",
        "\n",
        "            if prediction == category:\n",
        "                correct += 1\n",
        "\n",
        "        print(category, \" : \", (correct / len(test_data[category])))\n",
        "        overall_correct += correct\n",
        "        overall_length += len(test_data[category])\n",
        "\n",
        "    print(overall_correct/ overall_length)\n",
        "    print()"
      ],
      "metadata": {
        "id": "1JNRR8g88hHc"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split train and test sets by the acts."
      ],
      "metadata": {
        "id": "Y8fHzoVGY1Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_play = []\n",
        "test_play = []\n",
        "\n",
        "for play_name in play_names:\n",
        "    train_play.append(play_info[play_name].spoken_lines[:play_info[play_name].lines_per_act[3]])\n",
        "    test_play.append(play_info[play_name].spoken_lines[play_info[play_name].lines_per_act[3]:])"
      ],
      "metadata": {
        "id": "rErGHEuY8eMd"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I am attempting to find the pair of the plays, that are the best distinguished by my naive bayes classifier."
      ],
      "metadata": {
        "id": "ezCLhOzqY74n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(8):\n",
        "    for j in range(i + 1, 8):\n",
        "        print(play_names[i])\n",
        "        print(play_names[j])\n",
        "        model = Naive_bayes_model()\n",
        "        model.fit([train_play[i], train_play[j]])\n",
        "        evaluate(model, [test_play[i], test_play[j]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-97Q_-3mBu6x",
        "outputId": "6efe222e-8d04-4026-99fa-0b50147ef883"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a_and_c.xml\n",
            "dream.xml\n",
            "0  :  0.8980769230769231\n",
            "1  :  0.4117647058823529\n",
            "0.6793650793650794\n",
            "\n",
            "a_and_c.xml\n",
            "hamlet.xml\n",
            "0  :  0.6096153846153847\n",
            "1  :  0.6914600550964187\n",
            "0.6573033707865169\n",
            "\n",
            "a_and_c.xml\n",
            "j_caesar.xml\n",
            "0  :  0.7423076923076923\n",
            "1  :  0.56\n",
            "0.6659217877094972\n",
            "\n",
            "a_and_c.xml\n",
            "macbeth.xml\n",
            "0  :  0.8288461538461539\n",
            "1  :  0.345088161209068\n",
            "0.6194111232279171\n",
            "\n",
            "a_and_c.xml\n",
            "merchant.xml\n",
            "0  :  0.7692307692307693\n",
            "1  :  0.6172839506172839\n",
            "0.7109004739336493\n",
            "\n",
            "a_and_c.xml\n",
            "othello.xml\n",
            "0  :  0.6365384615384615\n",
            "1  :  0.7227191413237924\n",
            "0.6811862835959221\n",
            "\n",
            "a_and_c.xml\n",
            "r_and_j.xml\n",
            "0  :  0.7423076923076923\n",
            "1  :  0.7214611872146118\n",
            "0.732776617954071\n",
            "\n",
            "dream.xml\n",
            "hamlet.xml\n",
            "0  :  0.3247058823529412\n",
            "1  :  0.9201101928374655\n",
            "0.7002606429192006\n",
            "\n",
            "dream.xml\n",
            "j_caesar.xml\n",
            "0  :  0.5129411764705882\n",
            "1  :  0.8506666666666667\n",
            "0.67125\n",
            "\n",
            "dream.xml\n",
            "macbeth.xml\n",
            "0  :  0.6094117647058823\n",
            "1  :  0.6926952141057935\n",
            "0.6496350364963503\n",
            "\n",
            "dream.xml\n",
            "merchant.xml\n",
            "0  :  0.48\n",
            "1  :  0.7592592592592593\n",
            "0.6008010680907877\n",
            "\n",
            "dream.xml\n",
            "othello.xml\n",
            "0  :  0.42823529411764705\n",
            "1  :  0.8998211091234347\n",
            "0.6961382113821138\n",
            "\n",
            "dream.xml\n",
            "r_and_j.xml\n",
            "0  :  0.42823529411764705\n",
            "1  :  0.8789954337899544\n",
            "0.657010428736964\n",
            "\n",
            "hamlet.xml\n",
            "j_caesar.xml\n",
            "0  :  0.8360881542699724\n",
            "1  :  0.584\n",
            "0.7502270663033606\n",
            "\n",
            "hamlet.xml\n",
            "macbeth.xml\n",
            "0  :  0.8829201101928374\n",
            "1  :  0.3249370277078086\n",
            "0.6856634016028496\n",
            "\n",
            "hamlet.xml\n",
            "merchant.xml\n",
            "0  :  0.7851239669421488\n",
            "1  :  0.5030864197530864\n",
            "0.6980952380952381\n",
            "\n",
            "hamlet.xml\n",
            "othello.xml\n",
            "0  :  0.6336088154269972\n",
            "1  :  0.5957066189624329\n",
            "0.6171206225680934\n",
            "\n",
            "hamlet.xml\n",
            "r_and_j.xml\n",
            "0  :  0.731404958677686\n",
            "1  :  0.6872146118721462\n",
            "0.7147766323024055\n",
            "\n",
            "j_caesar.xml\n",
            "macbeth.xml\n",
            "0  :  0.7626666666666667\n",
            "1  :  0.5365239294710328\n",
            "0.6463730569948186\n",
            "\n",
            "j_caesar.xml\n",
            "merchant.xml\n",
            "0  :  0.728\n",
            "1  :  0.6512345679012346\n",
            "0.6924177396280401\n",
            "\n",
            "j_caesar.xml\n",
            "othello.xml\n",
            "0  :  0.6346666666666667\n",
            "1  :  0.7978533094812165\n",
            "0.7323340471092077\n",
            "\n",
            "j_caesar.xml\n",
            "r_and_j.xml\n",
            "0  :  0.672\n",
            "1  :  0.7831050228310502\n",
            "0.7318573185731857\n",
            "\n",
            "macbeth.xml\n",
            "merchant.xml\n",
            "0  :  0.5642317380352645\n",
            "1  :  0.7685185185185185\n",
            "0.6560332871012483\n",
            "\n",
            "macbeth.xml\n",
            "othello.xml\n",
            "0  :  0.41309823677581864\n",
            "1  :  0.8425760286225402\n",
            "0.6642259414225942\n",
            "\n",
            "macbeth.xml\n",
            "r_and_j.xml\n",
            "0  :  0.5113350125944585\n",
            "1  :  0.8401826484018264\n",
            "0.6838323353293413\n",
            "\n",
            "merchant.xml\n",
            "othello.xml\n",
            "0  :  0.5185185185185185\n",
            "1  :  0.7942754919499105\n",
            "0.6930917327293318\n",
            "\n",
            "merchant.xml\n",
            "r_and_j.xml\n",
            "0  :  0.6172839506172839\n",
            "1  :  0.7579908675799086\n",
            "0.6981627296587927\n",
            "\n",
            "othello.xml\n",
            "r_and_j.xml\n",
            "0  :  0.6118067978533095\n",
            "1  :  0.7031963470319634\n",
            "0.6519558676028084\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair with the best accuracy is *Hamlet* and *Julius Caesar* with accuracy of *75.02%* of correctly classifying a line. The accuracy of recognising a *Hamlet* line correctly is *83.61%*, but it's only *58.4%* for recognising *Julius Caesar* line correctly.\n",
        "\n",
        "So I actually think the two most easily distinguishable plays are *Julius Caesar* and *Othello*, with accuracy of *63.46% and 79.79%*. The overall accuracy of line classification is *73.23%*.\n"
      ],
      "metadata": {
        "id": "EZG04QbVZp1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifying between all 8 plays**\n",
        "\n",
        "Here it can be nicely seen, how longer plays have greater accuracy."
      ],
      "metadata": {
        "id": "10q4RsHDdDd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Naive_bayes_model()\n",
        "model.fit(train_play)\n",
        "evaluate(model, test_play)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nw65o0Vcpy2",
        "outputId": "6f4c7dea-8fe1-4b88-a16e-12f4535baddd"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  :  0.3596153846153846\n",
            "1  :  0.1388235294117647\n",
            "2  :  0.4462809917355372\n",
            "3  :  0.32266666666666666\n",
            "4  :  0.09319899244332494\n",
            "5  :  0.28703703703703703\n",
            "6  :  0.37209302325581395\n",
            "7  :  0.45662100456621\n",
            "0.32651434643995747\n",
            "\n"
          ]
        }
      ]
    }
  ]
}